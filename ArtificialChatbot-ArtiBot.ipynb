{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MlokctptsOS_"
      },
      "outputs": [],
      "source": [
        "# Natural Language Toolkit\n",
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sVa_kKeVLAqy"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import adjusted_rand_score\n",
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "d-0TRJXlspUR"
      },
      "outputs": [],
      "source": [
        "# for Web Scraping articles - scraping is sort of the extraction of text from the web article\n",
        "!pip install newspaper3k"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "X58lW4kfs6wh"
      },
      "outputs": [],
      "source": [
        "#importing the libraries\n",
        "from newspaper import Article\n",
        "import random\n",
        "import string\n",
        "import nltk\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "3FqX3T0IuHAt"
      },
      "outputs": [],
      "source": [
        "# Punkt Sentence is a tokenizer that divides a text into a list of sentences by using an unsupervised algorithm to build a model for\n",
        "# abbreviation words, collocations, and words that start sentences. It must be trained on a large collection of plaintext in the\n",
        "# target language before it can be used.\n",
        "nltk.download('punkt', quiet=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNQqjRI10cc-"
      },
      "source": [
        "Getting the authentic training-dataset below from tomshardware"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Bjjw_Ibeveew"
      },
      "outputs": [],
      "source": [
        "# Getting entire article each in one variable for catering linuxOS queries (from https://tomshardware.com/):\n",
        "import re\n",
        "\n",
        "article_links = [\n",
        "                 'https://www.tomshardware.com/how-to/check-disk-usage-linux',\n",
        "                 'https://www.tomshardware.com/how-to/kill-process-linux',\n",
        "                 'https://www.tomshardware.com/how-to/find-files-linux',\n",
        "                 'https://www.tomshardware.com/how-to/open-tar-files-linux',\n",
        "                 'https://www.tomshardware.com/how-to/zip-files-in-linux',\n",
        "                 'https://www.tomshardware.com/how-to/write-bash-scripts-linux',\n",
        "                 'https://forums.tomshardware.com/threads/recommended-used-laptop-for-linux.3734259/',\n",
        "                 'https://www.tomshardware.com/how-to/change-file-directory-permissions-linux',\n",
        "                 'https://www.tomshardware.com/how-to/monitor-cpu-and-ram-in-linux', \n",
        "                 'https://www.tomshardware.com/how-to/move-remove-files-linux',\n",
        "                 'https://www.tomshardware.com/how-to/change-passwords-in-linux',\n",
        "                 'https://www.tomshardware.com/how-to/add-remove-linux-software-using-apt',\n",
        "                 'https://www.tomshardware.com/how-to/run-windows-11-and-mac-os-virtual-machines-in-linux',\n",
        "                 'https://www.tomshardware.com/how-to/send-files-to-trash-linux',\n",
        "                 'https://www.tomshardware.com/how-to/kill-linux-processes-with-fkill',\n",
        "                 'https://www.tomshardware.com/how-to/grep-command-linux',\n",
        "                 'https://www.tomshardware.com/how-to/send-files-to-trash-linux',\n",
        "                 'https://www.tomshardware.com/how-to/getting-to-know-the-linux-filesystem',\n",
        "                ]\n",
        "\n",
        "NUM_ARTICLES = len(article_links) \n",
        "articles = []\n",
        "corpus = []\n",
        "\n",
        "for i in range(NUM_ARTICLES):\n",
        "  articles.append(Article(article_links[i]))\n",
        "  articles[i].download()\n",
        "  articles[i].parse()\n",
        "  articles[i].nlp()\n",
        "  #removing punctuations:\n",
        "  corpus.append(articles[i].text)\n",
        "  corpus[i] = re.sub('[%s]' % re.escape(string.punctuation), '', corpus[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdhgOh9zNt2P"
      },
      "source": [
        "Definition for stemming and lemmatization function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "5toFdPFFM0oz"
      },
      "outputs": [],
      "source": [
        "NUMWORDS_WORDNET = 155327\n",
        "\n",
        "#stemming:\n",
        "from nltk.stem import PorterStemmer\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def stem_words(text):\n",
        "    return \" \".join([stemmer.stem(word) for word in text.split()])\n",
        "\n",
        "# lemmatization:\n",
        "def lemmatize_words(text):\n",
        "    return \" \".join([lemmatizer.lemmatize(word) for word in text.split()])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8WUGQDfOFyu"
      },
      "source": [
        "Processing for the titles of articles in order to find the most appropriate article:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "d5ZTuRo0PCMC"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "znbstp2TOFZS"
      },
      "outputs": [],
      "source": [
        "#extracting articles titles from the url:\n",
        "from urllib.request import urlopen\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "a = []\n",
        "\n",
        "for k in range(NUM_ARTICLES):\n",
        "  # target url\n",
        "  url = article_links[k]\n",
        "\n",
        "  # using the BeautifulSoup module\n",
        "  soup = BeautifulSoup(urlopen(url))\n",
        "  a.append(soup.title.get_text())\n",
        "\n",
        "#stemming the extracted article titles:\n",
        "for j in range(NUM_ARTICLES):\n",
        "  a[j] = re.sub('[%s]' % re.escape(string.punctuation), '', a[j])\n",
        "  a[j] = stem_words(a[j])\n",
        "\n",
        "#lemmatization\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizr = WordNetLemmatizer()\n",
        "b = []\n",
        "\n",
        "for cnt in range(NUM_ARTICLES):\n",
        "  b.append(lemmatize_words(a[cnt]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ZNMRJx0NaQLJ"
      },
      "outputs": [],
      "source": [
        "x = []\n",
        " \n",
        "for cnt in range(NUM_ARTICLES):\n",
        "  x.append(stem_words(corpus[cnt]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "DM4WjnlYa3at"
      },
      "outputs": [],
      "source": [
        "#lemmatization\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "y = []\n",
        "\n",
        "for cnt in range(NUM_ARTICLES):\n",
        "  y.append(lemmatize_words(x[cnt]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "9Owkq21CQZn9"
      },
      "outputs": [],
      "source": [
        "sentence = []\n",
        "z = []\n",
        "\n",
        "for i in range(NUM_ARTICLES):\n",
        "  sentence.append(y[i])\n",
        "  tf_idf_vec_smooth = TfidfVectorizer(use_idf=True,  \n",
        "                        smooth_idf=True,  \n",
        "                        ngram_range=(1,1),stop_words='english')\n",
        "  tf_idf_data_smooth = tf_idf_vec_smooth.fit_transform([sentence[i]])\n",
        "  tf_idf_dataframe_smooth=pd.DataFrame(tf_idf_data_smooth.toarray(),columns=tf_idf_vec_smooth.get_feature_names())\n",
        "  z.append(tf_idf_dataframe_smooth)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "fESu16mdl42v"
      },
      "outputs": [],
      "source": [
        "# for article links:\n",
        "sentence2 = []\n",
        "c = []\n",
        "\n",
        "for i in range(NUM_ARTICLES):\n",
        "  sentence2.append(b[i])\n",
        "  tf_idf_vec_smooth3 = TfidfVectorizer(use_idf=True,  \n",
        "                        smooth_idf=True,  \n",
        "                        ngram_range=(1,1),stop_words='english')\n",
        "  tf_idf_data_smooth3 = tf_idf_vec_smooth3.fit_transform([sentence2[i]])\n",
        "  tf_idf_dataframe_smooth3=pd.DataFrame(tf_idf_data_smooth3.toarray(),columns=tf_idf_vec_smooth3.get_feature_names())\n",
        "  c.append(tf_idf_dataframe_smooth3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "uzGBOPj78AUb"
      },
      "outputs": [],
      "source": [
        "# function to return a random greeting response to a user's greeting:\n",
        "def responseGreeting(greeting):\n",
        "  greeting = greeting.lower()\n",
        "  greeting = ''.join(char for char in greeting if char.isalnum())   # ignoring special characters from the user's greeting\n",
        "\n",
        "  bot_greetings = [\"This is your friend Artibot. Howdy?\", \"Hey! How can I help you mate?\", \"Hello! I am here to help you my friend.\", \"Hola! I am here to assist you.\"]\n",
        "  user_greetings = [\"hello\", \"hi\", \"hey\", \"heya\", \"howdy\", \"i am stuck\", \"help me\", \"hellow\"]\n",
        "\n",
        "  for word in greeting.split():\n",
        "      if word in user_greetings:\n",
        "        return random.choice(bot_greetings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kuQc2RlWKJzd"
      },
      "source": [
        "Function to return the bot's response by finding the most appropriate answer (last function to edit):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "NevasSOu3AiL"
      },
      "outputs": [],
      "source": [
        "import math as mt\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "\n",
        "def botResponse(user_input):\n",
        "  bot_response = ''\n",
        "  tmp = ''\n",
        "  response_flag = 0\n",
        "  probs = []\n",
        "  haveWord = 0\n",
        "  haveWord2 = 0\n",
        "  tf_idf = []\n",
        "  tf = []\n",
        "\n",
        "  tf_idf2 = []\n",
        "  tf2 = []\n",
        "\n",
        "  idf = 0\n",
        "\n",
        "  largest = -1\n",
        "  largest2 = -1\n",
        "  \n",
        "  ind = -1\n",
        "  ind2 = -1\n",
        "\n",
        "  bagWords = []\n",
        "\n",
        "  user_input = re.sub('[%s]' % re.escape(string.punctuation), '', user_input)\n",
        "  xd =lemmatize_words(user_input)  \n",
        "  yd=stem_words(xd)\n",
        "\n",
        "  tf_idf_vec_smooth2 = TfidfVectorizer(use_idf=True,  \n",
        "                        smooth_idf=True,  \n",
        "                        ngram_range=(1,1),stop_words='english')\n",
        "  \n",
        "  tf_idf_data_smooth2 = tf_idf_vec_smooth2.fit_transform([yd])\n",
        "  tf_idf_dataframe_smooth2=pd.DataFrame(tf_idf_data_smooth2.toarray(),columns=tf_idf_vec_smooth2.get_feature_names())\n",
        "  z2=tf_idf_dataframe_smooth2\n",
        "  bagWords = z2.columns.tolist()\n",
        "\n",
        "  for i in range(NUM_ARTICLES):\n",
        "    for cnt in range(len(bagWords)):\n",
        "      wrd = bagWords[cnt]\n",
        "      if wrd in c[i].columns:\n",
        "        tf.append(c[i][wrd].iloc[0])\n",
        "        response_flag = 1\n",
        "      else:\n",
        "        tf.append(0)\n",
        "    if wrd in b[i]:\n",
        "      haveWord = haveWord+1\n",
        "\n",
        "  if haveWord != 0:\n",
        "    idf = NUM_ARTICLES/haveWord\n",
        "    idf = mt.log(idf)\n",
        "\n",
        "  for j in range(NUM_ARTICLES):\n",
        "    valTmp = idf*tf[j]\n",
        "    tf_idf.append(valTmp)\n",
        "    if valTmp>largest:\n",
        "      largest = valTmp\n",
        "      ind = j\n",
        "\n",
        "  print(\"INDEX = \", str(ind))\n",
        "\n",
        "  if response_flag and largest>0.8:\n",
        "    tmp = articles[ind]\n",
        "    articles[ind].download()\n",
        "    articles[ind].parse()\n",
        "    articles[ind].nlp()\n",
        "    txt = articles[ind].text\n",
        "    return txt\n",
        "\n",
        "  else:       # if not found by the article names, finding by the content of the articles:\n",
        "    haveWord = 0\n",
        "    response_flag = 0\n",
        "    for i in range(NUM_ARTICLES):\n",
        "      for cnt in range(len(bagWords)):\n",
        "        wrd = bagWords[cnt]\n",
        "        if wrd in z[i].columns:\n",
        "          tf2.append(z[i][wrd].iloc[0])\n",
        "          response_flag = 1\n",
        "        else:\n",
        "          tf2.append(0)\n",
        "      if wrd in corpus[i]:\n",
        "        haveWord = haveWord+1\n",
        "\n",
        "    if haveWord != 0:\n",
        "      idf = NUM_ARTICLES/haveWord\n",
        "      idf = mt.log(idf)\n",
        "\n",
        "    ind2 = -1\n",
        "    for j in range(NUM_ARTICLES):\n",
        "      valTmp = idf*tf2[j]\n",
        "      tf_idf2.append(valTmp)\n",
        "      if valTmp>largest2:\n",
        "        largest2 = valTmp\n",
        "        ind2 = j\n",
        "\n",
        "    print(\"INDEX2 = \", str(ind2))\n",
        "\n",
        "    if response_flag and largest2>largest:        # comparing the tf_idf of articleName and the content\n",
        "      tmp = articles[ind2]\n",
        "      articles[ind2].download()\n",
        "      articles[ind2].parse()\n",
        "      articles[ind2].nlp()\n",
        "      txt = articles[ind2].text\n",
        "      return txt\n",
        "    elif largest2>0.5:\n",
        "      tmp = articles[ind]\n",
        "      articles[ind].download()\n",
        "      articles[ind].parse()\n",
        "      articles[ind].nlp()\n",
        "      txt = articles[ind].text\n",
        "      return txt\n",
        "    else:                                          \n",
        "      return \"Oops, I apologize. I am unable to get any solution :(\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svVke1w7KQ-F"
      },
      "source": [
        "Start the chat:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "83xF1tYVgCA1"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output \n",
        "\n",
        "print(\"ArtiBot.ai: I am Dr.Torvalds' Intelligent ChatBot. I can answer your queries about LinuxOS.\")\n",
        "print(\"If you want to exit, let me know.\")\n",
        "\n",
        "exitList = [\"bye\", \"exit\", \"goodbye\", \"okay bye\", \"thanks bye\", \"get lost\", \"shut up\", \"good bye\", \"thanks\", \"see you\"]\n",
        "exitfg = False\n",
        "\n",
        "while(True):\n",
        "  userMsg = input(\"\\nYou: \")\n",
        "  tmp = userMsg\n",
        "  userMsg = userMsg.lower()\n",
        "  clear_output()\n",
        "\n",
        "  print(\"\\nYou: \"+tmp) \n",
        "  if userMsg in exitList:\n",
        "    print(\"ArtiBot.ai: Good Bye, Have a Nice Day!\")\n",
        "    break\n",
        "  else:\n",
        "    if responseGreeting(userMsg) != None:\n",
        "      print(\"ArtiBot.ai: \"+ responseGreeting(userMsg))\n",
        "    else:\n",
        "      print(\"ArtiBot.ai: \"+ botResponse(userMsg))\n",
        "  "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "ArtiBot.ai - ArtificialChatbot_AIProject - DONE.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}